from dataclasses import dataclass
from typing import Iterable, Sequence

import torch
from tokenizers import Tokenizer
from torch import nn

from .constants import DEFAULT_DEVICE


def make_inputs(
    tokenizer: Tokenizer,
    prompts: Sequence[str],
    device: torch.device = DEFAULT_DEVICE,
    add_pad_token: bool = True,
) -> dict[str, torch.Tensor]:
    ensure_tokenizer_has_pad_token(tokenizer, add_pad_token=add_pad_token)
    return tokenizer(prompts, padding=True, return_tensors="pt").to(device)


def ensure_tokenizer_has_pad_token(
    tokenizer: Tokenizer, add_pad_token: bool = True
) -> None:
    # from https://github.com/huggingface/transformers/issues/12594#issuecomment-877358955
    if not tokenizer.pad_token:
        if add_pad_token:
            tokenizer.pad_token = tokenizer.eos_token
        else:
            raise ValueError("Tokenizer must have a pad token")


def predict_from_input(
    model: nn.Module,
    inp: dict[str, torch.Tensor],
    answer_id_overrides: list[tuple[int, int]] = [],
) -> tuple[torch.Tensor, torch.Tensor]:
    probs = predict_probs_from_input(model, inp)
    prob, pred = torch.max(probs, dim=1)
    for i, j in answer_id_overrides:
        pred[i] = j
        prob[i] = probs[i, j]
    return pred, prob


def predict_probs_from_input(
    model: nn.Module,
    inp: dict[str, torch.Tensor],
) -> torch.Tensor:
    logits = predict_logits_from_input(model, inp)
    return torch.softmax(logits, dim=-1)


def predict_logits_from_input(
    model: nn.Module,
    inp: dict[str, torch.Tensor],
) -> torch.Tensor:
    all_logits = model(**inp)["logits"]
    final_token_positions = find_final_attention_positions(inp["attention_mask"])
    batch_indices = torch.arange(all_logits.size(0))
    return all_logits[batch_indices, final_token_positions]


def predict_next_tokens_greedy(
    model: nn.Module,
    tokenizer: Tokenizer,
    prompts: list[str],
    num_tokens: int = 1,
    device: torch.device = DEFAULT_DEVICE,
) -> list[list[int]]:
    """
    Greedily predict the next N tokens for each prompt in the list.
    Should correctly handle right-padding.
    """
    next_prompts = [*prompts]  # copy to avoid modifying the original
    results: list[list[int]] = []
    for _i in range(num_tokens):
        # decoding and then re-encoding in a loop is wasteful, but it's the easiest way to handle
        # batches of different lengths, since model.generate() doesn't work with right-padding
        inputs = make_inputs(
            tokenizer,
            next_prompts,
            device=device,
        )
        pred_res = predict_from_input(model, inputs)
        for j, pred in enumerate(pred_res[0].detach().cpu()):
            if j >= len(results):
                results.append([])
            results[j].append(pred.item())
            next_prompts[j] += tokenizer.decode(pred)
    return results


def find_final_attention_positions(attention_mask: torch.Tensor) -> torch.Tensor:
    # convoluted, and generated by ChatGPT, but seems to work
    indices = torch.arange(attention_mask.size(1)).to(attention_mask.device)
    # use broadcasting to expand indices to the shape of attention_mask
    indices = indices[None, :].expand_as(attention_mask)
    # set indices where attention_mask is 0 to -1
    indices = torch.where(attention_mask == 1, indices, -1)
    # find the max indices
    max_indices = indices.max(dim=1).values
    return max_indices


def get_answer_token_ids(
    tokenizer: Tokenizer,
    answer: str,
    ensure_space_prefix: bool = True,
    strip_start_token: bool = True,
    strip_blank_start_token: bool = True,
) -> list[int]:
    """
    Helper to find the token ids for the given answer as if it were a continuation of the prompt.
    """
    processed_answer = answer
    if ensure_space_prefix and not processed_answer.startswith(" "):
        processed_answer = " " + processed_answer
    tokens = tokenizer.encode(processed_answer)
    if strip_start_token and tokens[0] == tokenizer.bos_token_id:
        tokens = tokens[1:]
    # llama only includes an explicit space token at the start of the string if it's the first token
    if strip_blank_start_token and tokenizer.decode([tokens[0]]) == "":
        tokens = tokens[1:]
    return tokens


def any_answer_matches_expected(
    answers: Iterable[str], expected_answers: Iterable[str], exact_match: bool = True
) -> bool:
    """
    Check if any of the given answers match any of the expected answers. Handles case and whitespace.
    """
    for answer in answers:
        if answer_matches_expected(answer, expected_answers, exact_match=exact_match):
            return True
    return False


def answer_matches_expected(
    answer: str, expected_answers: Iterable[str], exact_match: bool = True
) -> bool:
    """
    Check if the given answer matches any of the expected answers. Handles case and whitespace.
    """
    processed_answer = process_answer(answer, exact_match)
    return processed_answer in {
        process_answer(a, exact_match) for a in expected_answers
    }


def process_answer(answer: str, exact_match: bool = True) -> str:
    """
    Process the given answer to make it easier to compare to other answers by removing case and trimming.
    """
    processed_answer = answer.strip()
    if not exact_match:
        processed_answer = processed_answer.lower()
    return processed_answer


@dataclass
class PromptAnswerData:
    answer_start_index: int
    answer: str
    answer_tokens: list[int]
    base_prompt: str
    full_prompt: str

    @property
    def output_answer_token_indices(self) -> tuple[int, ...]:
        # everything is shifted 1 earlier for output tokens
        output_start_index = self.answer_start_index - 1
        return tuple(
            range(output_start_index, output_start_index + len(self.answer_tokens))
        )


def find_prompt_answer_data(
    tokenizer: Tokenizer, base_prompt: str, answer: str
) -> PromptAnswerData:
    """
    Find the number of tokens in the given answer, after it's appended to the prompt.
    This assumes that the answer immediately follows the prompt
    NOTE: the prompt SHOULD NOT include the answer
    """
    base_prompt_stripped = base_prompt.strip()
    answer_stripped = answer.strip()
    full_prompt = base_prompt_stripped + " " + answer_stripped
    base_prompt_tokens = tokenizer.encode(base_prompt_stripped)
    full_prompt_tokens = tokenizer.encode(full_prompt)
    return PromptAnswerData(
        answer=answer_stripped,
        answer_start_index=len(base_prompt_tokens),
        answer_tokens=full_prompt_tokens[len(base_prompt_tokens) :],
        base_prompt=base_prompt_stripped,
        full_prompt=full_prompt,
    )
